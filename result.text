<Experiment 1>

print(generation_fitness)
print(sum(generation_time))
print(generation_score)
print(generation_failure_rate)
print(best_pop[-1])
print(tot_score[np.argmax(fitness)])

GA trace = 1

[0.7255, 0.6965, 0.7175, 0.6805, 0.694, 0.7165, 0.7725, 0.7305, 0.721, 0.728, 0.684, 0.765, 0.733, 0.8055]
50979.18000000001
[0.8175, 0.804, 0.8015, 0.8035, 0.8205, 0.849, 0.895, 0.888, 0.8775, 0.8715, 0.838, 0.903, 0.929, 0.9585]
[0.0181, 0.0265, 0.0724, 0.0224, 0.0307, 0.0384, 0.0259, 0.0058, 0.0071, 0.0038, 0.0086, 0.0086, 0.0, 0.0]
{'bucketing': 15, 'encoding': 'index', 'drop_act': 8, 'LightGBM': {'max_depth': 10, 'num_leaves': 325, 'min_child_samples': 5}}
0.98


<Experiment 2>

GA trace = 2

[0.7345, 0.725, 0.7475, 0.699, 0.699]
19118.884100000003
[0.812, 0.8182, 0.797, 0.8084, 0.808]
[0.052, 0.0518, 0.0428, 0.0095, 0.0329]
{'bucketing': 2, 'encoding': 'index', 'drop_act': 4, 'Xgboost': {'max_depth': 19, 'n_estimators': 39, 'learning_rate': 0.1}}
0.9981

second trial (experiment 2)

[0.682, 0.639, 0.691, 0.7905, 0.8485, 0.8565, 0.861, 0.8335, 0.8345, 0.8975, 0.8725, 0.941, 0.946, 0.778, 0.7715, 0.7345, 0.814]
48350.143899999995
[0.8354, 0.8052, 0.835, 0.8861, 0.924, 0.9309, 0.9409, 0.9179, 0.9125, 0.9459, 0.9687, 0.9771, 0.9797, 0.9803, 0.9803, 0.9804, 0.9805]
[0.09, 0.2184, 0.1215, 0.0528, 0.0262, 0.0143, 0.0042, 0.0143, 0.0256, 0.0167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
{'bucketing': 12, 'encoding': 'index', 'drop_act': 2, 'models': 'Decision Tree', 'LightGBM': {'max_depth': 9, 'num_leaves': 326, 'min_child_samples': 3}}
0.9808

random search

Best Score: 0.9644813956521302
Best Hyperparameters: {'min_samples_leaf': 5, 'max_depth': 20, 'criterion': 'entropy'}

print(elapsed_time)
print(score)

275.4128
[0.7996, 0.7815, 0.868, 0.8894, 0.9012, 0.9017, 0.9101, 0.9274, 0.9398, 0.9434, 0.9403, 0.9473]

