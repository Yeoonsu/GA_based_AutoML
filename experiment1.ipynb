{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 코드를 다 재작성하는게 아니라 그냥 돌아가게끔 예쁘게 편집만 하기\n",
    "only edit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data setting\n",
    "\n",
    "## BPIC2012\n",
    "caseid = 'Case ID'\n",
    "activity = 'Activity'\n",
    "ts = 'Complete Timestamp'\n",
    "label = {'Activity' : 'O_ACCEPTED-COMPLETE'}\n",
    "other_features = ['Resource', 'Variant index', '(case) AMOUNT_REQ']\n",
    "\n",
    "## BPIC2017\n",
    "# caseid = 'Case ID'\n",
    "# activity = 'Activity'\n",
    "# ts = 'Complete Timestamp'\n",
    "# label = {'column' : 'Accepted'}\n",
    "# other_features = ['Resource', 'CreditScore', 'FirstWithdrawalAmount', 'MonthlyCost', 'NumberOfTerms','OfferedAmount']\n",
    "\n",
    "## BPIC2015\n",
    "# caseid = 'Case ID'\n",
    "# activity = 'Activity'\n",
    "# ts = 'Complete Timestamp'\n",
    "# label = {'column' : 'Label'}\n",
    "# other_features = ['Resource', 'monitoringResource', '(case) Includes_subCases','(case) Responsible_actor','(case) caseProcedure','(case) caseStatus','(case) last_phase','(case) parts',\n",
    "#                 '(case) requestComplete','(case) termName', '(case) SUMleges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option setting\n",
    "\n",
    "combi = ['bucketing', 'encoding', 'drop_act', 'params']\n",
    "\n",
    "options = {\n",
    "    'bucketing' : (1,40), # a number of partitions\n",
    "    \n",
    "    'encoding' : ['index', 'aggregate'],\n",
    "    \n",
    "    'drop_act' : [2,4,6,8], # a number of activities to drop\n",
    "    \n",
    "    'models' : ['Decision Tree','Random Forest','LightGBM','Xgboost'],\n",
    "\n",
    "    'params' : {'Decision Tree':{'max_depth': (2,20),\n",
    "                           'min_samples_leaf': (5,100),\n",
    "                           'criterion': [\"gini\", \"entropy\"]\n",
    "            }, \n",
    "            'Random Forest':{\"n_estimators\": (10,1000), \n",
    "                           \"max_depth\": (2,20),\n",
    "                           \"max_features\": [\"auto\", \"log2\"], \n",
    "                           \"bootstrap\": [True, False],\n",
    "                           \"criterion\": [\"gini\", \"entropy\"]\n",
    "            },\n",
    "            'LightGBM':{'max_depth': (2,20),\n",
    "                      'num_leaves' : (10,500),\n",
    "                      'min_child_samples' : (2,10)\n",
    "            },\n",
    "            'Xgboost':{\"max_depth\": (2,20),\n",
    "                     \"n_estimators\": (10,1000),\n",
    "                     \"learning_rate\": [0.01, 0.05, 0.1]\n",
    "                     \n",
    "            }\n",
    "            }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import permutations\n",
    "import json, os\n",
    "from collections import OrderedDict\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label(df):       \n",
    "    Label = []\n",
    "    if 'Activity' in label:\n",
    "        label_col = label['Activity']                \n",
    "        for case, group in df.groupby(caseid):\n",
    "            for i in range(len(group)):\n",
    "                if label_col in group[activity].tolist():\n",
    "                    Label.append(1)\n",
    "                else:\n",
    "                    Label.append(0)\n",
    "        label_df = pd.DataFrame(Label, columns = ['Label'])\n",
    "        df = pd.concat([df, label_df], axis=1)\n",
    "\n",
    "    elif 'column' in label:\n",
    "        label_col = label['column']\n",
    "        df = df.rename(columns={label_col : 'Label'})\n",
    "    return df\n",
    "\n",
    "def drop_activity(df, n):\n",
    "    trace_num = df['Case ID'].nunique()\n",
    "    act = df['Activity'].value_counts()\n",
    "    df = df.iloc[[i for i in range(len(df)) if df.iloc[i]['Activity'] not in act[-n:]]]\n",
    "    return df\n",
    "\n",
    "def whole_bucket(df):\n",
    "    result = []\n",
    "    \n",
    "    for prefix in tqdm(range(2,42)):\n",
    "        bucket=[]\n",
    "        for case, group in df.groupby(caseid):\n",
    "            group = group.sort_values(by=ts, ascending = True).reset_index(drop=True)\n",
    "            if len(group) >= prefix:\n",
    "                bucket.append(group.iloc[:prefix,:])\n",
    "        new_df = pd.concat(bucket)\n",
    "        result.append(new_df)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def prefix_bound(m, drop_low=False):\n",
    "    if m == 1:\n",
    "        if drop_low == True:\n",
    "            return [[4,40]]\n",
    "        else:\n",
    "            return [[2,40]]\n",
    "    \n",
    "    if drop_low == True:\n",
    "        if m > 37:\n",
    "            m = 37\n",
    "        prefix_len = 37//m\n",
    "        remain = 37%m\n",
    "        prev = 4\n",
    "        bound=[]\n",
    "        for i in range(m):\n",
    "            if i < remain:\n",
    "                bound.append([prev,prev+prefix_len+1])\n",
    "                prev = prev+prefix_len+1\n",
    "            else:\n",
    "                bound.append([prev,prev+prefix_len])\n",
    "                prev = prev+prefix_len\n",
    "    else:  \n",
    "        prefix_len = 39//m\n",
    "        remain = 39%m\n",
    "        prev = 2\n",
    "        bound=[]\n",
    "        for i in range(m):\n",
    "            if i < remain:\n",
    "                bound.append([prev,prev+prefix_len+1])\n",
    "                prev = prev+prefix_len+1\n",
    "            else:\n",
    "                bound.append([prev,prev+prefix_len])\n",
    "                prev = prev+prefix_len\n",
    "        \n",
    "    return bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GA : Genetic Algorithm\n",
    "RS : Random Search\n",
    "\"\"\"\n",
    "df = pd.read_csv('BPIC12.csv')\n",
    "df = add_label(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# big data saving\n",
    "\n",
    "# df1 = drop_activity(df, 2)\n",
    "# df2 = drop_activity(df, 4)\n",
    "# df3 = drop_activity(df, 6)\n",
    "# df4 = drop_activity(df, 8)\n",
    "\n",
    "# df_list1 = whole_bucket(df1)\n",
    "# df_list2 = whole_bucket(df2)\n",
    "# df_list3 = whole_bucket(df3)\n",
    "# df_list4 = whole_bucket(df4)\n",
    "\n",
    "# with open('df_list1.pkl', 'wb') as f1:\n",
    "#     pickle.dump(df_list1, f1)\n",
    "    \n",
    "# with open('df_list2.pkl', 'wb') as f2:\n",
    "#     pickle.dump(df_list2, f2)\n",
    "    \n",
    "# with open('df_list3.pkl', 'wb') as f3:\n",
    "#     pickle.dump(df_list3, f3)\n",
    "    \n",
    "# with open('df_list4.pkl', 'wb') as f4:\n",
    "#     pickle.dump(df_list4, f4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Genetic_Algorithm:\n",
    "    def __init__(self, df, options : dict, combi : list , caseid : str, activity : str, ts : str, label : dict, other_features : list, phase):\n",
    "        self.df = df\n",
    "        self.options = options\n",
    "        self.combi = combi\n",
    "        self.caseid = caseid\n",
    "        self.activity = activity \n",
    "        self.ts = ts\n",
    "        self.label = label\n",
    "        self.other_features = other_features\n",
    "        \n",
    "             \n",
    "    def feature_type(self):\n",
    "        df = self.df\n",
    "        feature_dict = {}\n",
    "        for feature in self.other_features:\n",
    "            for case, group in df.groupby(self.caseid):\n",
    "                if len(group[feature].unique()) != 1:\n",
    "                    feature_dict[feature] = 'event'\n",
    "                    break\n",
    "                else:\n",
    "                    feature_dict[feature] = 'case'\n",
    "        self.feature_types = feature_dict\n",
    "        \n",
    "    def add_label(self, df):       \n",
    "        Label = []\n",
    "        if 'Activity' in self.label:\n",
    "            label_col = self.label['Activity']                \n",
    "            for case, group in df.groupby(self.caseid):\n",
    "                for i in range(len(group)):\n",
    "                    if label_col in group[self.activity].tolist():\n",
    "                        Label.append(1)\n",
    "                    else:\n",
    "                        Label.append(0)\n",
    "            label_df = pd.DataFrame(Label, columns = ['Label'])\n",
    "            df = pd.concat([df, label_df], axis=1)\n",
    "\n",
    "        elif 'column' in self.label:\n",
    "            label_col = self.label['column']\n",
    "            df = df.rename(columns={label_col : 'Label'})\n",
    "        self.df = df\n",
    "        return df\n",
    "                \n",
    "\n",
    "    def rand_choice(self, options, key : str):\n",
    "        if  type(options[key]) == tuple:\n",
    "            return np.random.randint(options[key][0], options[key][1])\n",
    "\n",
    "        elif type(options[key]) == list:\n",
    "            return options[key][np.random.randint(0, len(options[key]))]\n",
    "               \n",
    "        \n",
    "    def initial_populations(self, N : int, rand_state = 2022) -> dict:\n",
    "        initial_pop = []\n",
    "        if 'params' in self.combi:\n",
    "            for model in list(self.options['params'].keys()):\n",
    "                for n in range(N):\n",
    "                    result = {}\n",
    "                    result['bucketing'] = self.rand_choice(self.options, 'bucketing')\n",
    "                    result['encoding'] = self.rand_choice(self.options, 'encoding')\n",
    "                    result['drop_act'] = self.rand_choice(self.options, 'drop_act')\n",
    "                    result['models'] = model\n",
    "                    result[model] = {}\n",
    "                    for hp in list(self.options['params'][model].keys()):                             \n",
    "                        result[model][hp] = self.rand_choice(self.options['params'][model], hp)\n",
    "                    initial_pop.append(result)\n",
    "        else:\n",
    "            for n in range(N):\n",
    "                result = {}\n",
    "                result['bucketing'] = rand_choice(self.options, 'bucketing')\n",
    "                result['encoding'] = rand_choice(self.options, 'encoding')\n",
    "                result['drop_act'] = rand_choice(self.options, 'drop_act')\n",
    "                initial_pop.append(result)                \n",
    "        \n",
    "        self.population = initial_pop\n",
    "        \n",
    "        return initial_pop\n",
    "    \n",
    "    def select_population(self, population, fitness, N) -> list:\n",
    "#         population = self.population\n",
    "        sum_fit = sum(fitness)\n",
    "        selection_probs = [fitness[c]/sum_fit for c in range(len(population))]\n",
    "\n",
    "        return list(np.random.choice(population, int(N), p=selection_probs))\n",
    "\n",
    "    \n",
    "    # Roullette wheel selection\n",
    "    def select_param(self, population, fitness, hp : bool) -> list: \n",
    "#         population = self.population\n",
    "        sum_fit = sum(fitness)\n",
    "        selection_probs = [fitness[c]/sum_fit for c in range(len(fitness))]\n",
    "\n",
    "        if hp == False:\n",
    "            return np.random.choice(population, 2, p=selection_probs)\n",
    "\n",
    "        else:\n",
    "            p1 = np.random.choice(population, p=selection_probs)\n",
    "            hp_space = [c for c in range(len(population)) if list(p1.keys())[-1] in list(population[c].keys())]\n",
    "            hp_sum_fit = sum([fitness[c] for c in hp_space])\n",
    "            hp_selection_probs = [fitness[c]/hp_sum_fit for c in hp_space]\n",
    "            p2 = population[np.random.choice(hp_space, p=hp_selection_probs)]\n",
    "            return [p1, p2]\n",
    "        \n",
    "        \n",
    "    # cp : crossover probability ~ (0,1) -> 0.9\n",
    "    def crossover(self, population, fitness, num_offering : int, cp : float) -> dict:\n",
    "#         population = self.population\n",
    "        result = []\n",
    "        n = int(num_offering*cp)\n",
    "        hp_options = [True, False]\n",
    "        for _ in range(n):\n",
    "            hp_option = hp_options[np.random.randint(0, 2)]\n",
    "            child = {}\n",
    "            p1, p2 = self.select_param(population, fitness, hp=hp_option)\n",
    "            co_point = np.random.randint(low=0, high=len(p1))\n",
    "            for idx, key in enumerate(list(p2.keys())):\n",
    "                if idx < co_point:\n",
    "                    child[key] = p1[key]\n",
    "                else:\n",
    "                    child[key] = p2[key]\n",
    "            result.append(child)\n",
    "#         self.population = population.extend(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "\n",
    "    # mp : crossover probability ~ (0,1) -> 0.03\n",
    "    def mutation(self, population, N, mp : float):\n",
    "#         population = self.population\n",
    "        \n",
    "        n = int(N*mp)\n",
    "\n",
    "        result = []\n",
    "        for _ in range(n):\n",
    "            child = {}\n",
    "            parent = population[np.random.choice(len(population))]\n",
    "            params = [list(parent[key].keys())+[key] if key in list(self.options['params'].keys()) \n",
    "                      else key for key in list(parent.keys())]\n",
    "            params.extend(params.pop())\n",
    "\n",
    "            ml_model = params[-1]\n",
    "            mut_param = params[np.random.randint(low=0, high=len(params)-2)]\n",
    "            if mut_param in list(self.options.keys()):\n",
    "                parent[mut_param] = self.rand_choice(self.options, mut_param)\n",
    "            else:\n",
    "                parent[ml_model][mut_param] = self.rand_choice(self.options['params'][ml_model], mut_param)\n",
    "            result.append(parent)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def fitness(self, tot_score, failure_rate, tot_time):\n",
    "        highest_acc_pop = np.argmax(tot_score)\n",
    "\n",
    "        # fitness = (acc + (1-failure_rate) + time_cost + acc_decrease)/4\n",
    "        tot_fitness = [round((tot_score[i] + (1-failure_rate[i]) + (max(tot_time)-tot_time[i])/(max(tot_time)-min(tot_time)) \n",
    "                   + (tot_score[i]-min(tot_score))/(max(tot_score)-min(tot_score)))/4, 2) for i in range(len(tot_score))]\n",
    "        \n",
    "        return tot_fitness\n",
    "    \n",
    "        \n",
    "    def indexbased_encoding(self, df, prefix):\n",
    "        #df = self.df\n",
    "        new_df = pd.DataFrame()\n",
    "        for column in df.columns:\n",
    "            if (column == 'Label') or (column == self.caseid) or ((column in self.feature_types) and (self.feature_types[column] == 'case')):\n",
    "                case_df = df.groupby(self.caseid, as_index = False).first()[column]\n",
    "                new_df = pd.concat([new_df, case_df], axis=1)\n",
    "            elif (column == self.activity) or ((column in self.feature_types) and (self.feature_types[column] == 'event')):\n",
    "                col_name = [str(column)+str(i+1) for i in range(prefix)]\n",
    "                col_list = []\n",
    "                for case, group in df.groupby(self.caseid):\n",
    "                    group = group.sort_values(by=self.ts, ascending = True).reset_index(drop=True)\n",
    "                    col_list.append(group[column].tolist())\n",
    "                col_list = np.array(col_list)\n",
    "                event_df = pd.DataFrame(col_list, columns = col_name)\n",
    "                new_df = pd.concat([new_df, event_df], axis=1)\n",
    "\n",
    "        self.df = new_df\n",
    "        return self.one_hot_encoding(new_df)\n",
    "        \n",
    "    def aggregated_encoding(self, df):\n",
    "        # df = self.df\n",
    "        new_df = pd.DataFrame()\n",
    "        for column in df.columns:       \n",
    "            if (column == 'Label') or (column == self.caseid) or ((column in self.feature_types) and (self.feature_types[column] == 'case')):\n",
    "                case_df = df.groupby(self.caseid, as_index = False).first()[column]\n",
    "                new_df = pd.concat([new_df, case_df], axis=1)\n",
    "            elif (column == self.activity) or ((column in self.feature_types) and (self.feature_types[column] == 'case')):\n",
    "                result = []\n",
    "                if df[column].dtype == int or df[column].dtype == float:\n",
    "                    if (column in self.feature_types) and (self.feature_types[column] == 'case'):\n",
    "                        col_name = column\n",
    "                        case_df = df.groupby(self.caseid, as_index = False).first()\n",
    "                        not_nan = [num for num in list(case_df[column]) if num != np.nan]\n",
    "                        fir_point, sec_point = np.percentile(not_nan,[33,67])\n",
    "\n",
    "                        for val in case_df[column].values:\n",
    "                            if val < fir_point:\n",
    "                                result.append('Low')\n",
    "                            elif fir_point <= val < sec_point:\n",
    "                                result.append('Medium')\n",
    "                            elif val >= sec_point:\n",
    "                                result.append('High')\n",
    "                            else:\n",
    "                                result.append('Nan')\n",
    "                    else:\n",
    "                        col_name = [str(column)+'-'+point for point in ['Low', 'Medium', 'High', 'Nan']]\n",
    "                        not_nan = [num for num in list(df[column]) if num != np.nan]\n",
    "                        fir_point, sec_point = np.percentile(not_nan,[33,67])\n",
    "                        for case, group in df.groupby(self.caseid):\n",
    "                            col_list = [0]*len(col_name)\n",
    "                            for val in group[column].values:\n",
    "                                if val < fir_point:\n",
    "                                    col_list[0] += 1\n",
    "                                elif fir_point <= val < sec_point:\n",
    "                                    col_list[1] += 1\n",
    "                                elif val >= sec_point:\n",
    "                                    col_list[2] += 1\n",
    "                                else:\n",
    "                                    col_list[3] += 1\n",
    "                            result.append(col_list)\n",
    "                                    \n",
    "                else:\n",
    "                    col_name = df[column].unique()\n",
    "\n",
    "                    for case, group in df.groupby(self.caseid):\n",
    "                        group = group.sort_values(by=self.ts, ascending = True).reset_index(drop=True)\n",
    "                        col_list = [0]*len(col_name)\n",
    "                        key = list(group[column].value_counts().keys())\n",
    "                        val = group[column].value_counts().values\n",
    "                        for k in key:\n",
    "                            col_list[key.index(k)] += val[key.index(k)]\n",
    "                        result.append(col_list)\n",
    "\n",
    "                result = np.array(result)\n",
    "                event_df = pd.DataFrame(result, columns = col_name)\n",
    "                new_df = pd.concat([new_df, event_df], axis=1)\n",
    "        self.df = new_df\n",
    "        return new_df\n",
    "\n",
    "    \n",
    "    def last_state_encoding(self, df, window):\n",
    "        # df = self.df        \n",
    "        event = {}\n",
    "        caseid = []\n",
    "        next_event = []\n",
    "        for k in range(window):\n",
    "            dict_index = 'event{}'.format(k+1)\n",
    "            event[dict_index] = []\n",
    "            for case, group in df.groupby(self.caseid):\n",
    "                df1 = list(group[self.activity])\n",
    "                L = len(df1) - window + k\n",
    "                for j in range(k,L):\n",
    "                    event[dict_index].append(df1[j])\n",
    "                    if k == window -1:\n",
    "                        caseid.append(case)\n",
    "                        next_event.append(df1[j+1])\n",
    "        \n",
    "        df_1 = pd.DataFrame(caseid, columns = [self.caseid])\n",
    "        df_2 = pd.DataFrame.from_dict(event)\n",
    "        df_3 = pd.DataFrame(next_event, columns = ['Label'])\n",
    "        new_df = pd.concat([df_1, df_2, df_3], axis=1)        \n",
    "        self.df = new_df\n",
    "        \n",
    "        return new_df\n",
    "    \n",
    "    \n",
    "    def encoding(self, df, key, prefix):\n",
    "        if key == \"index\":\n",
    "            return self.indexbased_encoding(df, prefix)\n",
    "        \n",
    "        elif key == \"aggregate\":\n",
    "            return self.aggregated_encoding(df)\n",
    "        \n",
    "        else:\n",
    "            return self.last_state_encoding(df, 3)\n",
    "    \n",
    "    def one_hot_encoding(self, df):\n",
    "        #df = self.df\n",
    "        for column in df.columns:\n",
    "            if not np.issubdtype(df[column], np.number):\n",
    "                one_hot = pd.get_dummies(df[column], prefix=column, prefix_sep='=')\n",
    "                #print(\"Encoded column:{} - Different keys: {}\".format(column, one_hot.shape[1]))\n",
    "                df = df.drop(column, axis=1)\n",
    "                df = df.join(one_hot)\n",
    "        #print(\"Categorical columns encoded\")\n",
    "        self.df = df\n",
    "        return df\n",
    "     \n",
    "    \n",
    "    def train_test_set_split(self, df, encoding):\n",
    "                \n",
    "        df_train, df_test = train_test_split(df, test_size=0.2, random_state=1, shuffle=False)\n",
    "#         print(f'Training samples: {len(df_train)} \\nTest samples: {len(df_test)}')\n",
    "        X_train = df_train.drop('Label', axis=1)\n",
    "        y_train = df_train['Label']\n",
    "        X_test = df_test.drop('Label', axis=1)\n",
    "        y_test = df_test['Label']\n",
    "        \n",
    "        if encoding == 'last_state':\n",
    "            ohe = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
    "            ohe.fit(X_train.values)\n",
    "            X_train = ohe.transform(X_train.values)\n",
    "            X_test = ohe.transform(X_test.values)\n",
    "\n",
    "            #X_train = self.one_hot_encoding(X_train)\n",
    "            #X_test = self.one_hot_encoding(X_test)\n",
    "        \n",
    "        else:\n",
    "            ratio = len(df_train[df_train['Label'] == 1]) / len(df_train[df_train['Label'] == 0])     \n",
    "#             print(f'Ratio of target in training set 0 : 1 = 1:{ratio:.4f}')\n",
    "\n",
    "            # For imbalanced data\n",
    "            if ratio < 0.33:\n",
    "                sm = SMOTE(random_state=0)\n",
    "                sm_X_train, sm_y_train = sm.fit_resample(X_train, y_train)\n",
    "                print('After OverSampling, the shape of train_X: {}'.format(sm_X_train.shape))\n",
    "                print('After OverSampling, the shape of train_y: {} \\n'.format(sm_y_train.shape))\n",
    "                print(\"After OverSampling, counts of label '1': {}\".format(sum([sm_y_train[i]==1 for i in range(len(sm_y_train))])))\n",
    "                print(\"After OverSampling, counts of label '0': {}\".format(sum([sm_y_train[i]==0 for i in range(len(sm_y_train))])))\n",
    "                X_train = sm_X_train\n",
    "                y_train = sm_y_train\n",
    "                            \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "        return X_train, y_train, X_test, y_test\n",
    "     \n",
    "    def model_fit(self):\n",
    "        #['Logistic Regression','Decision Tree','Random Forest','LightGBM','Xgboost','CatBoost']\n",
    "        models = {'Decision Tree' : DecisionTreeClassifier(), 'Random Forest' : RandomForestClassifier(), 'LightGBM' : LGBMClassifier(), 'Xgboost' : XGBClassifier()}\n",
    "        tot_score = []\n",
    "        for model in models:\n",
    "            score = []\n",
    "\n",
    "            model.fit(self.X_train, self.y_train)\n",
    "\n",
    "            score.append(accuracy_score(self.y_test, model.predict(self.X_test)))\n",
    "            score.append(precision_score(self.y_test, model.predict(self.X_test)))\n",
    "            score.append(recall_score(self.y_test, model.predict(self.X_test)))\n",
    "            score.append(f1_score(self.y_test, model.predict(self.X_test)))\n",
    "            score.append(roc_auc_score(self.y_test, model.predict_proba(self.X_test)[:, 1]))\n",
    "\n",
    "            print(f'Model is {model} \\nAccuracy: {score[0]:.4f}, Precision: {score[1]:.4f}, Recall: {score[2]:.4f}, F1: {score[3]:.4f}, AUC: {score[4]:.4f}')\n",
    "            tot_score.append(score)\n",
    "\n",
    "        self.tot_score = tot_score\n",
    "\n",
    "        # plot score df\n",
    "        score_df = pd.DataFrame(tot_score, index = models, columns = ['Accuracy', 'Precision', 'Recall', 'F1 score', 'AUC'])\n",
    "        score_df.plot(kind=\"bar\",figsize=(9,8))\n",
    "        plt.xticks(rotation='horizontal')\n",
    "        plt.show()\n",
    "\n",
    "        self.score_df = score_df\n",
    "        return\n",
    "    \n",
    "    def decision_tree(self, hp, X_train, y_train, X_test, y_test):\n",
    "        model = DecisionTreeClassifier(max_depth = hp['max_depth'],\n",
    "                           min_samples_leaf= hp['min_samples_leaf'],\n",
    "                           criterion = hp['criterion'])\n",
    "        model.fit(X_train, y_train)\n",
    "        score  = round((accuracy_score(y_test, model.predict(X_test))+roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))/2, 4)\n",
    "        return score\n",
    "    \n",
    "    def random_forest(self, hp, X_train, y_train, X_test, y_test):\n",
    "        model = RandomForestClassifier(n_estimators=hp['n_estimators'], \n",
    "                           max_depth= hp[\"max_depth\"],\n",
    "                           max_features= hp[\"max_features\"], \n",
    "                           bootstrap= hp[\"bootstrap\"],\n",
    "                           criterion= hp[\"criterion\"])\n",
    "        model.fit(X_train, y_train)\n",
    "        score  = round((accuracy_score(y_test, model.predict(X_test))+roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))/2, 4)\n",
    "        return score\n",
    "    \n",
    "    def light_gbm(self, hp, X_train, y_train, X_test, y_test):\n",
    "        model = LGBMClassifier(max_depth= hp[\"max_depth\"],\n",
    "                           num_leaves= hp[\"num_leaves\"], \n",
    "                           min_child_samples= hp[\"min_child_samples\"])\n",
    "        model.fit(X_train, y_train)\n",
    "        score  = round((accuracy_score(y_test, model.predict(X_test))+roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))/2, 4)\n",
    "        return score\n",
    "    \n",
    "    def xgboost(self, hp, X_train, y_train, X_test, y_test):\n",
    "        model = XGBClassifier(max_depth = hp[\"max_depth\"],\n",
    "                           n_estimators = hp[\"n_estimators\"], \n",
    "                           learning_rate = hp[\"learning_rate\"])\n",
    "        model.fit(X_train, y_train)\n",
    "        score  = round((accuracy_score(y_test, model.predict(X_test))+roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))/2, 4)\n",
    "        return score\n",
    "        \n",
    "    def select_best_model(self):\n",
    "        score_df = self.score_df\n",
    "        arg_index = np.argmax(score_df['Accuracy'] + score_df['AUC'])\n",
    "        print(f'Best model is {score_df.index[arg_index]} \\nAccuracy: {score_df.iloc[arg_index][0]:.4f}, Precision: {score_df.iloc[arg_index][1]:.4f}, Recall: {score_df.iloc[arg_index][2]:.4f}, F1: {score_df.iloc[arg_index][3]:.4f}, AUC: {score_df.iloc[arg_index][4]:.4f}')\n",
    "    \n",
    "    \n",
    "    def main(self):\n",
    "        \n",
    "        populations = self.initial_populations(20)\n",
    "        tot_score = []\n",
    "        result_df, prefix_idx = whole_bucket(df)\n",
    "        for pop in populations:\n",
    "            df1 = drop_activity(df, pop['drop_act'])\n",
    "            bucket_list = bucketing(result_df, prefix_idx, pop['bucketing'])\n",
    "            for bucket in bucket_list:\n",
    "                score = []\n",
    "                df1 = result_df[bucket[0]:bucket[1]]\n",
    "                df1 = self.encoding(df1, pop['encoding'])\n",
    "                X_train, y_train, X_test, y_test = self.train_test_set_split(df1, pop['encoding'])\n",
    "                if 'Decision Tree' in pop:\n",
    "                    score.append(self.decision_tree(pop['Decision Tree'], X_train, y_train, X_test, y_test))\n",
    "                elif 'Random Forest' in pop:\n",
    "                    score.append(self.random_forest(pop['Random Forest'], X_train, y_train, X_test, y_test))\n",
    "                elif 'LightGBM' in pop:\n",
    "                    score.append(self.light_gbm(pop['LightGBM'], X_train, y_train, X_test, y_test))\n",
    "                else: \n",
    "                    score.append(self.xgboost(pop['Xgboost'], X_train, y_train, X_test, y_test))\n",
    "                tot_score.append(score)\n",
    "        print(tot_score)        \n",
    "        new_df, prefix_idx = whole_bucket(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./df_list1.pkl', 'rb') as f:\n",
    "    df_list1 = pickle.load(f)\n",
    "    \n",
    "with open('./df_list2.pkl', 'rb') as f:\n",
    "    df_list2 = pickle.load(f)\n",
    "    \n",
    "with open('./df_list3.pkl', 'rb') as f:\n",
    "    df_list3 = pickle.load(f)\n",
    "    \n",
    "with open('./df_list4.pkl', 'rb') as f:\n",
    "    df_list4 = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selection rate\n",
    "sr = 0.1\n",
    "#crossover rate\n",
    "cr = 0.9\n",
    "#mutation rate\n",
    "mr = 0.01\n",
    "\n",
    "max_iter = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GA : Genetic Algorithm\n",
    "RS : Random Search\n",
    "\"\"\"\n",
    "phase = 1 # drop_activity(GA) + encoding(GA) + bucketing(GA) + ML params(GA) + hyperparameter opt(GA)\n",
    "# phase = 2 # drop_activity(GA) + encoding(GA) + bucketing(GA) + ML params(GA) + hyperparameter opt(RS)\n",
    "# phase = 3 # drop_activity(GA) + encoding(GA) + bucketing(GA) + ML params(RS) + hyperparameter opt(RS)\n",
    "# phase = 4 # drop_activity(RS) + encoding(RS) + bucketing(RS) + ML params(RS) + hyperparameter opt(RS)\n",
    "\n",
    "GA = Genetic_Algorithm(df, options, combi, caseid, activity, ts, label, other_features, phase)\n",
    "GA.feature_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bucketing': 20, 'encoding': 'index', 'drop_act': 4, 'models': 'Decision Tree', 'Decision Tree': {'max_depth': 8, 'min_samples_leaf': 68, 'criterion': 'entropy'}}\n",
      "bucket :  (2, 4)\n",
      "After OverSampling, the shape of train_X: (34594, 11)\n",
      "After OverSampling, the shape of train_y: (34594,) \n",
      "\n",
      "After OverSampling, counts of label '1': 17297\n",
      "After OverSampling, counts of label '0': 17297\n",
      "bucket :  (4, 6)\n",
      "After OverSampling, the shape of train_X: (23472, 19)\n",
      "After OverSampling, the shape of train_y: (23472,) \n",
      "\n",
      "After OverSampling, counts of label '1': 11736\n",
      "After OverSampling, counts of label '0': 11736\n",
      "bucket :  (6, 8)\n",
      "bucket :  (8, 10)\n",
      "bucket :  (10, 12)\n",
      "bucket :  (12, 14)\n",
      "bucket :  (14, 16)\n",
      "bucket :  (16, 18)\n",
      "bucket :  (18, 20)\n",
      "bucket :  (20, 22)\n",
      "bucket :  (22, 24)\n",
      "bucket :  (24, 26)\n",
      "bucket :  (26, 28)\n",
      "bucket :  (28, 30)\n",
      "bucket :  (30, 32)\n",
      "bucket :  (32, 34)\n",
      "bucket :  (34, 36)\n",
      "bucket :  (36, 38)\n",
      "bucket :  (38, 40)\n",
      "bucket :  (40, 41)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [04:12<1:19:54, 252.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bucketing': 17, 'encoding': 'index', 'drop_act': 2, 'models': 'Decision Tree', 'Decision Tree': {'max_depth': 3, 'min_samples_leaf': 38, 'criterion': 'gini'}}\n",
      "bucket :  (2, 5)\n",
      "After OverSampling, the shape of train_X: (47026, 17)\n",
      "After OverSampling, the shape of train_y: (47026,) \n",
      "\n",
      "After OverSampling, counts of label '1': 23513\n",
      "After OverSampling, counts of label '0': 23513\n",
      "bucket :  (5, 8)\n",
      "After OverSampling, the shape of train_X: (32868, 48)\n",
      "After OverSampling, the shape of train_y: (32868,) \n",
      "\n",
      "After OverSampling, counts of label '1': 16434\n",
      "After OverSampling, counts of label '0': 16434\n",
      "bucket :  (8, 11)\n",
      "bucket :  (11, 14)\n",
      "bucket :  (14, 17)\n",
      "bucket :  (17, 19)\n",
      "bucket :  (19, 21)\n",
      "bucket :  (21, 23)\n",
      "bucket :  (23, 25)\n",
      "bucket :  (25, 27)\n",
      "bucket :  (27, 29)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [08:18<2:37:44, 498.14s/it]\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "\n",
    "# #selection rate\n",
    "# sr_param = [0.1, 0.2, 0.3]\n",
    "# #crossover rate\n",
    "# cr_param = [0.9, 0.8, 0.7]\n",
    "# #mutation rate\n",
    "# mr_param = [0.1, 0.05, 0.01]\n",
    "\n",
    "best_pop = []\n",
    "# param_test = {'fitness':[], 'time':[], 'score':[], 'failure rate':[]}\n",
    "\n",
    "populations = GA.initial_populations(5)\n",
    "\n",
    "# for i in range(3):\n",
    "#     sr = sr_param[i]\n",
    "#     cr = cr_param[i]\n",
    "#     for j in range(3):\n",
    "#         print('trial =',i+j+1)\n",
    "#         mr = mr_param[j]\n",
    "generation_fitness = []\n",
    "generation_score = []\n",
    "generation_time = []\n",
    "generation_failure_rate = []\n",
    "for n_iter in range(max_iter):\n",
    "    new_population = []\n",
    "    tot_score = []\n",
    "    tot_time = []\n",
    "    failure_rate = []\n",
    "    for pop in tqdm(populations):\n",
    "        start_time = time.time()\n",
    "        new_population.append(pop)\n",
    "        print(pop)\n",
    "        if pop['drop_act'] == 2:\n",
    "            df_list = df_list1\n",
    "        elif pop['drop_act'] == 4:\n",
    "            df_list = df_list2\n",
    "        elif pop['drop_act'] == 6:\n",
    "            df_list = df_list3\n",
    "        else:\n",
    "            df_list = df_list4\n",
    "\n",
    "        if pop['encoding'] == 'last_state':\n",
    "            if pop['bucketing'] < 5:\n",
    "                pop['bucketing'] = 5\n",
    "            bound_list = prefix_bound(pop['bucketing'], drop_low=True)\n",
    "        else:\n",
    "            bound_list = prefix_bound(pop['bucketing'])\n",
    "\n",
    "        score = []\n",
    "        for bounds in bound_list:\n",
    "            lower, upper = bounds\n",
    "            merge_df = pd.DataFrame()\n",
    "            print('bucket : ',(lower, upper))\n",
    "            for idx in range(lower, upper):\n",
    "                prefix_df = df_list[idx-2]\n",
    "                prefix_df = GA.encoding(prefix_df, pop['encoding'], idx)\n",
    "                merge_df = pd.concat([merge_df, prefix_df], sort=False)\n",
    "\n",
    "            merge_df = merge_df.fillna(0)\n",
    "\n",
    "            X_train, y_train, X_test, y_test = GA.train_test_set_split(merge_df, pop['encoding'])    \n",
    "\n",
    "            #start = time.time()\n",
    "            if 'Decision Tree' in pop:\n",
    "                score.append(GA.decision_tree(pop['Decision Tree'], X_train, y_train, X_test, y_test))\n",
    "            elif 'Random Forest' in pop:\n",
    "                score.append(GA.random_forest(pop['Random Forest'], X_train, y_train, X_test, y_test))\n",
    "            elif 'LightGBM' in pop:\n",
    "                score.append(GA.light_gbm(pop['LightGBM'], X_train, y_train, X_test, y_test))\n",
    "            else: \n",
    "                score.append(GA.xgboost(pop['Xgboost'], X_train, y_train, X_test, y_test))                    \n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        elapsed_time = round(end_time-start_time,2)\n",
    "\n",
    "        tot_time.append(elapsed_time)\n",
    "        tot_score.append(round(sum(score)/len(score),2))\n",
    "\n",
    "        min_proba = 0.7\n",
    "        failure_rate.append(len([i for i in score if i < min_proba])/len(score))\n",
    "\n",
    "    fitness = GA.fitness(tot_score, failure_rate, tot_time)\n",
    "    best_pop.append(populations[np.argmax(fitness)])\n",
    "\n",
    "    N = len(new_population)\n",
    "    pop1 = GA.select_population(new_population, fitness, N*sr)\n",
    "    pop2 = GA.crossover(new_population, fitness, N, cr)\n",
    "    pop3 = GA.mutation(new_population, N, mr)\n",
    "\n",
    "    populations = pop1 + pop2 + pop3\n",
    "\n",
    "    generation_fitness.append(round(sum(fitness)/len(fitness),4))\n",
    "    generation_time.append(sum(tot_time))\n",
    "    generation_score.append(round(sum(tot_score)/len(tot_score),4))\n",
    "    generation_failure_rate.append(round(sum(failure_rate)/len(failure_rate),4))\n",
    "    \n",
    "    if cnt > 5:\n",
    "        break\n",
    "    \n",
    "    elif len(generation_fitness) > 1:\n",
    "        if abs(generation_fitness[-1]-generation_fitness[-2]) < 0.001:\n",
    "            break \n",
    "        elif (generation_fitness[-1]-generation_fitness[-2]) < 0:\n",
    "            cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generation_fitness)\n",
    "print(sum(generation_time))\n",
    "print(generation_score)\n",
    "print(generation_failure_rate)\n",
    "print(best_pop[-1])\n",
    "print(tot_score[np.argmax(fitness)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89ea80d57eb15db2b2c34f3c90a65f136145061d7e5ac43992953d1b221026f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
